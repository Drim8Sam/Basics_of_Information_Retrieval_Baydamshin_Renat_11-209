# InformationRetrievalBasic
## Baydamshin Renat, group 11-209

---
# Deployment Manual

## Requirements

- Python 3.10+ (рекомендуется 3.11+)
- pip

## Dependencies

Установить зависимости:

```bash
pip install requests
```

Запуск:

```bash
python main.py
```

# Release Notes 

## Web Crawler Homework (1st Task)

Этот проект представляет собой простой веб-краулер на Python, написанный в рамках учебного задания. Он скачивает HTML-страницы из предварительно заданного списка, сохраняет их в текстовые файлы и создает индексный файл.

- Скачивает 100 HTML-страниц из списка `urls.json`.
- Сохраняет содержимое каждой страницы (вместе с HTML-разметкой) в отдельный файл в папке `crawled_pages`.
- Создает файл `index.txt`, где каждая строка содержит номер файла и соответствующий URL.

После выполнения программы в директории проекта появятся:
Папка `crawled_pages/`, содержащая файлы `1.txt`, `2.txt`, ..., каждый из которых — содержимое соответствующей скачанной страницы.
Файл `index.txt`, в котором каждая строка имеет вид: `<номер_файла> <URL>`.

Также был написан метод, который очищает скаченные страницы, от тегов `script`, `style`, `link`, `noscript`, `iframe`, `meta`. При этом сохраняет html структуру страницы для последующих задач.
